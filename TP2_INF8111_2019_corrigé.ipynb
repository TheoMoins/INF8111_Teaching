{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF8111 - Fouille de données\n",
    "\n",
    "\n",
    "## TP2 Automne 2019 - Extraction et analyse d'une base de données de tweets\n",
    "\n",
    "##### Membres de l'équipe:\n",
    "\n",
    "    - Nom (Matricule) 1\n",
    "    - Nom (Matricule) 2\n",
    "    - Nom (Matricule) 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation du problème\n",
    "\n",
    "En 2017, Twitter compte 313 millions d’utilisateurs actifs par mois avec 500 millions de tweets envoyés par jour. Cette information est rendue disponible à destination de la recherche et du développement web grâce à une API publique qui permet de collecter les informations que l'on souhaite.\n",
    "\n",
    "Néanmoins, la politique de développement de Twitter limite le partage de ces données. En effet, le partage du contenu des tweets dans une base de données n'est pas autorisé, seuls les identifiants des tweets le sont. \n",
    "Pour partager publiquement une base de données de tweets que l'on a créée, il faut que cette base de données ne soit consituée que des identifiants de tweets, et c'est ce que l'on retrouve dans la plupart des jeux de données publiques.\n",
    "\n",
    "Il est donc nécessaire pour exploiter ces données \"d'hydrater\" les tweets en question, c'est-à-dire extraire l'ensemble des informations à partir de l'ID, ce qui demande d'utiliser l'API de Twitter.\n",
    "\n",
    "Nous allons ici utiliser des bases de données publiques créées par GWU (George Washington University), qui ont l'avantage d'être très récentes : \n",
    "https://dataverse.harvard.edu/dataverse/gwu-libraries\n",
    "\n",
    "Chaque base de données de GWU couvre un sujet précis (élection américaine de 2016, jeux olympiques, etc.), et les données ont été recueillis en appliquant des requêtes qui filtraient les résultats pour n'avoir que des tweets pertinents. Un fichier README est fourni avec chaque base de données pour donner les détails de création du *dataset*. \n",
    "\n",
    "\n",
    "**Les objectifs de ce TP sont donc les suivants :**\n",
    "\n",
    " 1. Construire un *crawler* qui collecte les informations d'un tweet à partir de son ID, avec le jeu de données de son choix et les informations pertinentes pour le sujet choisi\n",
    " 2. A partir de ces données de Twitter collectés, application de méthodes en Machine Learning (ML)/Natural Language Processing (NLP) pour fournir une analyse pertinente. \n",
    "\n",
    "\n",
    "Twitter autorisant le partage **local** des données (par exemple au sein d'un groupe de recherche), une base de données sera fournie si vous ne parvenez pas à créer la vôtre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/ Hydratation de tweets à l'aide de l'API Twitter (4 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obtenir l'authorisation de Twitter pour l'utilisation de l'API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'authentification, Twitter utilise OAuth : https://developer.twitter.com/en/docs/basics/authentication/overview/oauth\n",
    "Vous aurez ici besoin en particulier de OAuth2, car vous n'allez pas interagir avec des utilisateurs sur Twitter (simplement collectés des données).\n",
    "\n",
    "##### 1.1. Obtention d'un compte Twitter développeur\n",
    "\n",
    " La première étape nécessaire pour enregistrer votre application et de créer un compte Twitter développeur. Pour ce faire :\n",
    "\n",
    " - Créez-vous un compte Twitter classique\n",
    " \n",
    " - Sur le site, https://developer.twitter.com, cliquez sur *apply* pour obtenir un compte développeur. \n",
    " \n",
    " - Remplissez tous les champs nécessaires. Twitter demande beaucoup de détails sur l'utilisation que vous allez faire de ce compte, il est donc important d'expliquer la démarche en détail : il faut souligner le fait que le projet est **académique** (aucune intention commerciale, aucune publication des données collectés, etc.), expliquer les objectifs et l'apprentissage de ce TP (prise en main de l'API Twitter, l'application concrète de méthodes de Data Mining, etc.), mais aussi expliquer en détail ce que vous allez faire des données, les méthodes que vous allez appliquer, le rendu fourni, etc.  Si jamais vous n'êtes pas assez précis, Twitter peut vous renvoyer un courriel pour vous demander des précisions. \n",
    "\n",
    "##### 1.2. Obtention d'un jeton d'accès\n",
    "\n",
    " - Lorsque Twitter aura validé votre demande de compte développeur, allez sur https://developer.twitter.com/en/apps pour créer une application (cliquer sur *create an app*)\n",
    "\n",
    "- Ici encore, des informations sont à fournir ici. Certaines, comme le nom ou le site internet, ne sont pas très importante, vous pouvez mettre un site internet factice si vous le souhaitez.\n",
    "\n",
    "- A la fin de ce processus, vous pouvez enfin obtenir les clés et les jetons pour utiliser l'API: allez sur la page de l'application pour créer les jetons. Vous devez récupérer une paire de clés et une paire de jetons pour passer à la suite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_KEY = \"FcXQoS4quLXk4IWlVHQYEBucR\"\n",
    "CONSUMER_SECRET = \"zDyWsFuv1xkseWmwyNYiuMfGuQtcNyhH0W8XvaTx5MAsdQRdJe\"\n",
    "\n",
    "oauth_token = \"1161714912730767366-f3M1WyAc5iJmYISnuP58wZ9lRFaFas\"\n",
    "oauth_secret = \"LiRdoXPaS5jJAmYTcJLqUJy0BZjouDTpsHG1UWS0c0snB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Premiers pas avec Twython\n",
    "\n",
    "##### 2.1 Installation et import de la librairie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plusieurs librairies Python existent pour manipuler l'API Twitter. Aussi appelé *wrappers*, ce sont un ensemble de fonctions python qui appelle des fonctions de l'API. Parmi elles, nous utiliserons Twython, librairie répendue et activement maintenue.\n",
    "\n",
    "Documentation de Twython : https://twython.readthedocs.io/en/latest/api.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from twython import Twython, TwythonError, TwythonRateLimitError\n",
    "except ImportError:\n",
    "    !pip install twython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Création d'une application et premiers tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, oauth_token, oauth_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un test avec une recherche très simple pour vous assurer que la requête fonctionne.\n",
    "\n",
    "La fonction search renvoie une recherche (non exhaustive) de tweets, et l'option \"*popular*\" permet de retourner les résultats les plus populaires de la réponse. (documentation ici: https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_search = twitter.search(q='python', result_type='popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `search` renvoie un dictionnaire contenant la liste de tweets de la requête, et les métadonnées.\n",
    "\n",
    "Voici un exemple d'un résultat d'une recherche, observez ainsi toutes les données/métadonnées que contient un tweet et que vous pouvez extraire par la suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': 'Thu Aug 22 16:52:07 +0000 2019',\n",
       " 'id': 1164581023360978945,\n",
       " 'id_str': '1164581023360978945',\n",
       " 'text': '...when someone laughs at themselves, do they hurt themselves ? Apparently not\\n\\nRT : So you disapprove of PC ?\\n\\nMe:… https://t.co/mzo0gvOWAL',\n",
       " 'truncated': True,\n",
       " 'entities': {'hashtags': [],\n",
       "  'symbols': [],\n",
       "  'user_mentions': [],\n",
       "  'urls': [{'url': 'https://t.co/mzo0gvOWAL',\n",
       "    'expanded_url': 'https://twitter.com/i/web/status/1164581023360978945',\n",
       "    'display_url': 'twitter.com/i/web/status/1…',\n",
       "    'indices': [117, 140]}]},\n",
       " 'metadata': {'result_type': 'popular', 'iso_language_code': 'en'},\n",
       " 'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',\n",
       " 'in_reply_to_status_id': None,\n",
       " 'in_reply_to_status_id_str': None,\n",
       " 'in_reply_to_user_id': None,\n",
       " 'in_reply_to_user_id_str': None,\n",
       " 'in_reply_to_screen_name': None,\n",
       " 'user': {'id': 10810102,\n",
       "  'id_str': '10810102',\n",
       "  'name': 'John Cleese',\n",
       "  'screen_name': 'JohnCleese',\n",
       "  'location': 'London',\n",
       "  'description': 'Yes, I am still indeed alive, contrary to rumour, and am performing the silly walk in my new app http://t.co/16QGv879Ew',\n",
       "  'url': 'http://t.co/cp8Gw99xPO',\n",
       "  'entities': {'url': {'urls': [{'url': 'http://t.co/cp8Gw99xPO',\n",
       "      'expanded_url': 'http://www.johncleese.com',\n",
       "      'display_url': 'johncleese.com',\n",
       "      'indices': [0, 22]}]},\n",
       "   'description': {'urls': [{'url': 'http://t.co/16QGv879Ew',\n",
       "      'expanded_url': 'http://www.thesillywalk.com',\n",
       "      'display_url': 'thesillywalk.com',\n",
       "      'indices': [97, 119]}]}},\n",
       "  'protected': False,\n",
       "  'followers_count': 5684112,\n",
       "  'friends_count': 234,\n",
       "  'listed_count': 37495,\n",
       "  'created_at': 'Mon Dec 03 13:43:30 +0000 2007',\n",
       "  'favourites_count': 446,\n",
       "  'utc_offset': None,\n",
       "  'time_zone': None,\n",
       "  'geo_enabled': False,\n",
       "  'verified': True,\n",
       "  'statuses_count': 7001,\n",
       "  'lang': None,\n",
       "  'contributors_enabled': False,\n",
       "  'is_translator': False,\n",
       "  'is_translation_enabled': False,\n",
       "  'profile_background_color': '000000',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_tile': False,\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/2622284361/1emzqsaz3t5glbyndf66_normal.jpeg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/2622284361/1emzqsaz3t5glbyndf66_normal.jpeg',\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/10810102/1510162991',\n",
       "  'profile_link_color': '097112',\n",
       "  'profile_sidebar_border_color': '87BC44',\n",
       "  'profile_sidebar_fill_color': 'E0FF92',\n",
       "  'profile_text_color': '005523',\n",
       "  'profile_use_background_image': True,\n",
       "  'has_extended_profile': False,\n",
       "  'default_profile': False,\n",
       "  'default_profile_image': False,\n",
       "  'following': False,\n",
       "  'follow_request_sent': False,\n",
       "  'notifications': False,\n",
       "  'translator_type': 'none'},\n",
       " 'geo': None,\n",
       " 'coordinates': None,\n",
       " 'place': None,\n",
       " 'contributors': None,\n",
       " 'is_quote_status': False,\n",
       " 'retweet_count': 210,\n",
       " 'favorite_count': 1754,\n",
       " 'favorited': False,\n",
       " 'retweeted': False,\n",
       " 'lang': 'en'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_search['statuses'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est également possible avec Twython de récupérer les informations d'un tweet à partir de son ID. \n",
    "\n",
    "#### Question 1. Afficher la date, le nom d'utilisateur et le contenu du tweet ayant l'ID : 1157345692517634049\n",
    "\n",
    "*Indice : vous pourrez utiliser avec la fonction de twython `show_status`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date : Fri Aug 02 17:41:30 +0000 2019\n",
      "Name : Donald J. Trump\n",
      "Content : A$AP Rocky released from prison and on his way home to the United States from Sweden. It was a Rocky Week, get home ASAP A$AP!\n"
     ]
    }
   ],
   "source": [
    "test_id = \"1157345692517634049\"\n",
    "\n",
    "print(\"Date : \" + twitter.show_status(id=test_id)['created_at'])\n",
    "print(\"Name : \" + twitter.show_status(id=test_id)['user']['name'])\n",
    "print(\"Content : \" + twitter.show_status(id=test_id)['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention** : Twitter a une limitation de requête par fenêtre de 15 minutes, qui est donc à prendre en compte dans la base de données : https://developer.twitter.com/en/docs/basics/rate-limiting.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hydratation d'une base de donnée de tweets\n",
    "\n",
    "Les choses sérieuses commencent ! \n",
    "\n",
    "On souhaite désormais construire une fonction `hydrate_database` qui, à partir d'un fichier texte contenant une liste d'ID de tweets, créer un fichier csv contenant les informations que l'on souhaite extraire. \n",
    "\n",
    "Due à la limitation de requête, la fonction `show_status` vue plus haut s'avère peu efficace pour cette tâche : à raison de 900 requêtes pour 15 minutes, il sera beaucoup trop long de construire une base de données un tant soit peu conséquente. La fonction `lookup_status` (voir documentation) sera donc plus adaptée. Elle permettra d'hydrater 100 tweets par requête, ce qui, a raison d'une limite de 900 requêtes pour 15 minutes, rends la construction de la base de données plus réaliste. Il faudra tout de même gérer l'erreur générer par la limitation, si l'on souhaite avoir plus de 90000 tweets ou si l'on appelle plusieurs fois la fonction en moins de 15 minutes.\n",
    "\n",
    "#### Question 2. Implémenter la fonction `hydrate_database`\n",
    "\n",
    "*Attention : Il faut également gérer le cas où la feature demandée n'est pas une clé du dictionnaire mais une sous-clé, comme c'est le cas pour le nom d'utilisateur par exemple.*\n",
    "\n",
    "*Indice : La fonction `sleep` du module time permet de patienter le temps nécessaire*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TWEET_HYDRATATION_LIMIT = 100\n",
    "\n",
    "def hydrate_database(filename, database_name, \n",
    "                     features, nb_requests, \n",
    "                     tweet_hydratation_limit=100):\n",
    "    \n",
    "    file = open(filename, \"r\")\n",
    "\n",
    "    with open(database_name, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        i = 0 \n",
    "        \n",
    "        while i < nb_requests:\n",
    "\n",
    "            print(\"\\r{}/{}\".format(i+1, nb_requests),end=\"\")        \n",
    "\n",
    "            concat = \"\"\n",
    "            for _ in range(TWEET_HYDRATATION_LIMIT):\n",
    "                line = file.readline().strip()\n",
    "                concat += line + \",\"\n",
    "            try:\n",
    "                request = twitter.lookup_status(id=concat)\n",
    "                for tweet in request:\n",
    "                    csv_line = []\n",
    "                    for f in features:\n",
    "                        if len(f)==1:\n",
    "                            tweet_elt = tweet[f[0]]\n",
    "                        else:\n",
    "                            tweet_elt = tweet[f[0]][f[1]]\n",
    "                        if ',' in tweet_elt:\n",
    "                            tweet_elt = tweet_elt.replace(',', ' ')\n",
    "                        csv_line.append(tweet_elt)\n",
    "                        \n",
    "                    writer.writerow(csv_line)\n",
    "                i += 1\n",
    "                    \n",
    "            except TwythonError as e:\n",
    "                if isinstance(e, TwythonRateLimitError):\n",
    "                    retry_after = int(e.retry_after)\n",
    "                    now = int(time.time())\n",
    "                    timeToWait = retry_after - now + 1\n",
    "                    sys.stderr.write(\"Rate limit exceeded, sleeping for %ds.\\n\" % (timeToWait))\n",
    "                    time.sleep(timeToWait)\n",
    "                    print(\"Waking up after %ds\" % (int(time.time()) - now))\n",
    "\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez le fichier suivant en guise d'example : \n",
    "https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/5QCCUU/QPYP8G&version=1.1\n",
    "\n",
    "On suppose qu'on ne souhaite garder que le texte (*text*) l'ID de l'utilisateur (*user/screen_name*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100"
     ]
    }
   ],
   "source": [
    "filename = \"gwu/climate_id.txt\"\n",
    "database_name = \"databases/climate.csv\"\n",
    "features = [['text'], ['user', 'screen_name']]\n",
    "nb_requests = 100\n",
    "\n",
    "hydrate_database(filename, database_name, features, nb_requests, tweet_hydratation_limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II/ Analyse d'une base de données au choix (16 pts)\n",
    "\n",
    "Maintenant que vous êtes en mesure d'hydrater une base de données de tweets efficacement et en prenant en compte les limitations de Twitter, vous pouvez l'appliquer sur le *dataset* qui vous intéresse le plus.\n",
    "\n",
    "### 1. Instructions\n",
    "\n",
    "Dans cette partie, vous allez mener **entièrement** un projet de *Data Science*, de la collecte des données jusqu'à l'interprétation des résultats. Vous devez choisir parmi les 4 sujets suivants:\n",
    " \n",
    " 1. Analyse de sentiments pour la prédiction des résultats de l'élection américaine. \n",
    "    **Dataset**: \"2016 United States Presidential Election Tweet Ids\", https://doi.org/10.7910/DVN/PDI7IN  \n",
    " \n",
    " \n",
    " 2. Détection de discours d'incitation à la haine.\n",
    "    **Dataset**: \"Immigration and Travel Ban Tweet Ids\", https://doi.org/10.7910/DVN/5CFLLJ\n",
    " \n",
    " \n",
    " 3. Méthode de clustering appliqué au tweet sur l'actualité, et analyse des résultats. \n",
    "    **Dataset**: \"News Outlet Tweet Ids\", https://doi.org/10.7910/DVN/2FIFLH\n",
    "\n",
    " \n",
    " 4. Analyse de sentiment appliqué au changement climatique.\n",
    "    **Dataset**: \"Climate Change Tweets Ids\", https://doi.org/10.7910/DVN/5QCCUU\n",
    "    \n",
    "\n",
    "Vous êtes entièrement libre sur l'ensemble du processus (choix des informations extraites, méthodes en ML, librairie, etc.). Ces sujets étant populaires au sein de la communauté scientifique, vous pouvez (**si vous le souhaitez**) vous inspirer d'articles de la littérature, à condition de le citer dans votre rapport et de faire votre propre implémentation. \n",
    "\n",
    "#### L'objectif cependant ici n'est pas d'obtenir l'état de l'art, mais d'appliquer une méthodologie claire et rigoureuse que vous aurez construite vous-même. \n",
    "\n",
    "Les datasets étant massifs, il est fortement déconseillé de faire une base de données contenant tous les tweets hydratés (par exemple, les auteurs de la BDD n°1 soulignent qu'avec les limitations de l'API cela vous prendrait environ 32 jours). C'est à vous de voir quelle est la taille du dataset dont vous avez besoin.\n",
    "\n",
    "Si vous faites de l'apprentissage supervisé : vous allez avoir besoin d'entraîner un modèle avec un ensemble étiqueté, et donc deux solutions s'offrent à vous. Soit vous allez devoir récupérer des données étiquetées, soit vous êtes en mesure de labelliser vous-même vos données (par exemple, dans le cas du sujet n°1, la base de données est divisé en collections, et certaines dépendent du parti politique).\n",
    "\n",
    "Pensez aussi à lire le fichier README correspondant à la base que vous avez choisi, afin de vous aider à mieux comprendre vos futurs résultats.\n",
    "\n",
    "### 2. Rédaction d'un rapport\n",
    "\n",
    "Pour ce TP, vous allez devoir fournir un rapport qui détail et justifie l'ensemble de votre méthode, et qui fournisse les résultats que vous avez obtenus. Les éléments suivants doivent y apparaitre (cela peut vous servir de plan, mais ce n'est pas rigide) :\n",
    "\n",
    "- Titre du projet, et nom de l'ensemble des membres de l'équipe (avec mail et matricule)\n",
    "    \n",
    "- **Introduction** : résumé du problème, de la méthodologie et des résultats obtenus\n",
    "\n",
    "- **Présentation du dataset** : description, justification de la taille, du choix des features, etc. \n",
    "\n",
    "- **Preprocessing** : s'il y en a, justification des étapes de preprocessing  \n",
    "\n",
    "- **Methodologie** : description et justification de l'ensemble des choix (algorithmes, hyper-paramètres, régularisation, métriques, etc.)\n",
    "\n",
    "- **Résultats** : analyse des résultats obtenus (utilisez des figures pour illustrer), mise en relation entre les choix de design et la performance obtenue.\n",
    "\n",
    "- **Discussion** : discutez des avantages et des inconvénients de votre approche; quels sont les faiblesses, les failles ? Qu'est-ce qu'il peut être amélioré ? Vous pouvez également suggérer des futures idées d'exploration.\n",
    "\n",
    "- **Références** : si vous vous êtes inspiré d'une étude déjà faite\n",
    "    \n",
    "Vous pouvez utiliser le template d'arXiv pour le rapport : https://fr.overleaf.com/latex/templates/style-and-template-for-preprints-arxiv-bio-arxiv/fxsnsrzpnvwc. **L'ensemble du rapport ne doit cependant pas excéder 5 pages, figures et références compris.** Les 5 pages ne sont pas obligatoires, si vous estimez que moins est suffisant et que votre rapport est effectivement complet, vous ne serez pas pénalisé.\n",
    "\n",
    "\n",
    "### 3. Rendu attendu\n",
    "\n",
    "A la fin du TP, vous enverrez un fichier *zip* par mail (theo.moins@polymtl.ca) contenant les éléments suivants:\n",
    "\n",
    "- Le fichier *pdf* du rapport\n",
    "- Ce notebook que vous aurez complété. Vous pouvez également implémenter votre méthode à la suite ici, ou alors utiliser un autre fichier si vous le souhaitez (le code doit être commenté et clair).\n",
    "- Ne pas envoyer les fichiers de données, car trop conséquent. Avec le rapport et le code, tout sera détaillé et il sera possible de le refaire facilement.\n",
    "\n",
    "### 4. Evalutation\n",
    "\n",
    "75% de la note (soit 12 points) de cette partie sera basé sur la méthodologie, et 25% (soit 4 points) sur les résultats.\n",
    "\n",
    "La notation sur la méthodologie inclus : \n",
    "\n",
    "- La pertinence de l'ensemble des étapes de l'approche\n",
    "\n",
    "- La bonne description des algorithmes choisis\n",
    "\n",
    "- La justification judicieuse des choix établis\n",
    "\n",
    "- Une analyse pertinente des résultats\n",
    "\n",
    "- La clarté et l'organisation du rapport (figures, tables) et du code.\n",
    "\n",
    "\n",
    "Pour ce qui est des résultats, il est impossible de mettre un barème fixe car ils vont dépendre du sujet choisi. C'est un problème auquel vous allez être confrontés : chaque problème étant spécifique, il peut être compliqué d'évaluer qualitativement un modèle, d'autant que vous n'avez sans doute pas connaissance de l'état de l'art. C'est pourquoi il va être important de faire plusieurs essais, et de comparer différentes méthodes. Ainsi, les résultats doivent être cohérent avec la complexité de votre implémentation : un modèle simple et naïf vous fournira des premiers résultats, que vous devrez ensuite améliorer avec des modèles plus précis et complexes.\n",
    "\n",
    "De ce fait, l'ensemble des points pour les résultats seront donnés si : \n",
    " - Vous obtenez des premiers résultats avec une méthode naïve qui témoignent de la pertinence de vos choix \n",
    " - Ces résultats sont ensuite significativement améliorés avec une méthode plus complexe\n",
    " - Le tout est bien justifié et remis dans le contexte du problème "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sujet n°1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400"
     ]
    }
   ],
   "source": [
    "filename = \"gwu/democratic-party-timelines.txt\"\n",
    "database_name = \"databases/democratic-party-timelines.csv\"\n",
    "features = [['text']]\n",
    "nb_requests = 400\n",
    "\n",
    "hydrate_database(filename, database_name, features, nb_requests)\n",
    "\n",
    "filename = \"gwu/republican-party-timelines.txt\"\n",
    "database_name = \"databases/republican-party-timelines.csv\"\n",
    "features = [['text']]\n",
    "nb_requests = 400\n",
    "\n",
    "hydrate_database(filename, database_name, features, nb_requests, tweet_hydratation_limit=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set :  29848\n",
      "Length of validation set :  6552\n",
      "Length of test set :  6424\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "with open(\"databases/democratic-party-timelines.csv\", 'r', newline='', encoding=\"latin-1\") as csvfile:\n",
    "\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "    # Taking the header of the file + the index of useful columns:\n",
    "    header = next(reader)\n",
    "\n",
    "    for row in reader:\n",
    "        X.append(row[0])\n",
    "        y.append(0)\n",
    "        \n",
    "    assert len(X) == len(y)\n",
    "\n",
    "with open(\"databases/republican-party-timelines.csv\", 'r', newline='', encoding=\"latin-1\") as csvfile:\n",
    "\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "    # Taking the header of the file + the index of useful columns:\n",
    "    header = next(reader)\n",
    "\n",
    "    for row in reader:\n",
    "        X.append(row[0])\n",
    "        y.append(1)\n",
    "        \n",
    "    assert len(X) == len(y)\n",
    "    \n",
    "\n",
    "train_valid_X, test_X, train_valid_Y, test_Y = train_test_split(X, y, test_size=0.15, random_state=12)\n",
    "\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train_valid_X, train_valid_Y, test_size=0.18, random_state=12)\n",
    "\n",
    "print(\"Length of training set : \", len(train_X))\n",
    "print(\"Length of validation set : \", len(valid_X))\n",
    "print(\"Length of test set : \", len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class SpaceTokenizer(object):\n",
    "    \"\"\"\n",
    "    It tokenizes the tokens that are separated by whitespace (space, tab, newline). \n",
    "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
    "\n",
    "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Write your code here\n",
    "\n",
    "        # Have to return a list of tokens\n",
    "        return text.split()\n",
    "\n",
    "\n",
    "class NLTKTokenizer(object):\n",
    "    \"\"\"\n",
    "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Write your code here\n",
    "\n",
    "        tknzr = TweetTokenizer()\n",
    "\n",
    "        # Have to return a list of tokens\n",
    "        return tknzr.tokenize(text)\n",
    "\n",
    "\n",
    "class Stemmer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "    def stem(self, tokens):\n",
    "        \"\"\"\n",
    "        tokens: a list of strings\n",
    "        \"\"\"\n",
    "        # Have to return a list of stems\n",
    "        return self.stemmer.stem(tokens)\n",
    "\n",
    "\n",
    "class TwitterPreprocessing(object):\n",
    "\n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        tweet: original tweet\n",
    "        \"\"\"\n",
    "        new_tweet = []\n",
    "        # Write your preprocessing steps here.\n",
    "        for word in tweet:\n",
    "            # remove punctuation\n",
    "            if any(c in string.punctuation for c in word):\n",
    "                continue\n",
    "            # remove url and username\n",
    "            if '@' in word or 'http' in word:\n",
    "                continue\n",
    "            # remove word if digit inside\n",
    "            if any(letter.isdigit() for letter in word):\n",
    "                continue\n",
    "            # check if empty\n",
    "            if not word.strip():\n",
    "                continue\n",
    "            new_tweet.append(word.replace('#', ''))\n",
    "\n",
    "        # return the preprocessed twitter\n",
    "        return new_tweet\n",
    "\n",
    "class PreprocessingPipeline:\n",
    "\n",
    "    def __init__(self, tokenization, twitterPreprocessing, stemming):\n",
    "        \"\"\"\n",
    "        tokenization: enable or disable tokenization.\n",
    "        twitterPreprocessing: enable or disable twitter preprocessing.\n",
    "        stemming: enable or disable stemming.\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokenizer = NLTKTokenizer() if tokenization else SpaceTokenizer()\n",
    "        self.twitterPreprocesser = TwitterPreprocessing(\n",
    "        ) if twitterPreprocessing else None\n",
    "        self.stemmer = Stemmer() if stemming else None\n",
    "\n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        Transform the raw data\n",
    "\n",
    "        tokenization: boolean value.\n",
    "        twitterPreprocessing: boolean value. Apply the\n",
    "        stemming: boolean value.\n",
    "        \"\"\"\n",
    "\n",
    "        tokens = self.tokenizer.tokenize(tweet)\n",
    "\n",
    "        if self.stemmer:\n",
    "            tokens = list(map(self.stemmer.stem, tokens))\n",
    "\n",
    "        if self.twitterPreprocesser:\n",
    "            tokens = self.twitterPreprocesser.preprocess(tokens)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "\n",
    "def bigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    # Write your code here\n",
    "    # This function returns the list of bigrams\n",
    "    return [\" \".join(tokens[i:i + 2]) for i in range(len(tokens) - 1)]\n",
    "\n",
    "\n",
    "class TFIDFBoW(object):\n",
    "\n",
    "    def __init__(self, pipeline, bigram=False, trigram=False):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.words = None\n",
    "        self.idf = None\n",
    "    \n",
    "    def computeTFIDF(self, X):\n",
    "        \"\"\"\n",
    "        Calcule du TF-IDF, à partir d'un dictionnaire de mots et d'une \n",
    "        liste de tweets.\n",
    "        On suppose que l'on a déjà collecté le dictionnaire ainsi que \n",
    "        calculé le vecteur contenant l'idf pour chaque document.\n",
    "        \n",
    "        Entrée : X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "        \n",
    "        data, row, col = [], [], []\n",
    "        for i, tweet in enumerate(X):\n",
    "            for word in set(tweet):\n",
    "                if word in self.words:\n",
    "                    data.append(tweet.count(word) * self.idf[self.words.index(word)])\n",
    "                    row.append(i)\n",
    "                    col.append(self.words.index(word))\n",
    "\n",
    "        return csr_matrix((data, (row, col)), shape=(len(X), len(self.words)))\n",
    "\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
    "        si besoin, et transforme les textes en vecteurs d'entiers grâce à la méthode TF-IDF.\n",
    "        \n",
    "        Entrée : X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        X = list(map(self.pipeline.preprocess, X))\n",
    "        if self.bigram:\n",
    "            for tweet in X:\n",
    "                tweet += bigram(tweet)\n",
    "        if self.trigram:\n",
    "            for tweet in X:\n",
    "                tweet += trigram(tweet)\n",
    "\n",
    "        # list of all words\n",
    "        self.words = list(set((word for tweet in X for word in tweet)))\n",
    "\n",
    "        # compute idf\n",
    "        self.idf = []\n",
    "        for word in self.words:\n",
    "            count = sum([word in tweet for tweet in X])\n",
    "            self.idf.append(math.log(len(X) / count))\n",
    "\n",
    "        return self.computeTFIDF(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
    "        si besoin, et transforme les textes en vecteurs d'entiers grâce à la méthode TF-IDF.\n",
    "        Différence avec fit_transform : on suppose qu'on dispose déjà du dictionnaire et du calcul des idf ici.\n",
    "            \n",
    "        Entrée : X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "\n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "\n",
    "        X = list(map(self.pipeline.preprocess, X))\n",
    "        if self.bigram:\n",
    "            for tweet in X:\n",
    "                tweet += bigram(tweet)\n",
    "        if self.trigram:\n",
    "            for tweet in X:\n",
    "                tweet += trigram(tweet)\n",
    "\n",
    "        return self.computeTFIDF(X)\n",
    "    \n",
    "def train_evaluate(training_X, training_Y, validation_X, validation_Y, bowObj):\n",
    "    \"\"\"\n",
    "    training_X: tweets from the training dataset\n",
    "    training_Y: tweet labels from the training dataset\n",
    "    validation_X: tweets from the validation dataset\n",
    "    validation_Y: tweet labels from the validation dataset\n",
    "    bowObj: Bag-of-word object\n",
    "    \n",
    "    :return: the classifier and its accuracy in the training and validation dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    classifier = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "    training_rep = bowObj.fit_transform(training_X)\n",
    "\n",
    "    classifier.fit(training_rep, training_Y)\n",
    "\n",
    "    trainAcc = accuracy_score(training_Y, classifier.predict(training_rep))\n",
    "    validationAcc = accuracy_score(\n",
    "        validation_Y, classifier.predict(bowObj.transform(validation_X)))\n",
    "\n",
    "    return classifier, trainAcc, validationAcc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         3:  99.990%    96.627%  \n"
     ]
    }
   ],
   "source": [
    "# 3. TFIDFBoW + NLTKTokenizer + Stemming + unigram\n",
    "bowObj = TFIDFBoW(\n",
    "    PreprocessingPipeline(\n",
    "        tokenization=True, twitterPreprocessing=False, stemming=True))\n",
    "\n",
    "classifier, trainAcc, validAcc = train_evaluate(train_X, train_Y, valid_X,\n",
    "                                  valid_Y, bowObj)\n",
    "\n",
    "print(\"{:10}: {:^10.3%} {:^10.3%}\".format(3, trainAcc, validAcc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sujet n°2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sujet n°3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sujet n°4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
